{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir) \n",
    "\n",
    "PARENT_DIR = Path(parent_dir)\n",
    "\n",
    "# App\n",
    "# from dataloader.dali_loader import DaliLoader\n",
    "\n",
    "# Third Party\n",
    "import numpy as numpy\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "# Third Party\n",
    "import numpy as numpy\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "# NVIDIA\n",
    "from nvidia.dali.pipeline import Pipeline\n",
    "import nvidia.dali.ops as ops\n",
    "import nvidia.dali.types as types\n",
    "from nvidia.dali.plugin import pytorch\n",
    "\n",
    "\n",
    "class VideoReaderPipeline(Pipeline):\n",
    "    def __init__(self, batch_size, sequence_length, num_threads, device_id, file_root, crop_size, transforms=None, filenames=[]):\n",
    "        super(VideoReaderPipeline, self).__init__(batch_size, num_threads, device_id, seed=12)\n",
    "        self.reader = ops.VideoReader(\n",
    "            device='gpu',\n",
    "            file_root=file_root,\n",
    "            sequence_length=sequence_length,\n",
    "            normalized=True,\n",
    "            random_shuffle=True,\n",
    "            image_type=types.RGB,\n",
    "            dtype=types.UINT8,\n",
    "            initial_fill=16,\n",
    "#             filenames=filenames\n",
    "        )\n",
    "\n",
    "        self.crop = ops.Crop(device=\"gpu\", crop=crop_size, output_dtype=types.FLOAT)\n",
    "        self.transpose = ops.Transpose(device=\"gpu\", perm=[3, 0, 1, 2])\n",
    "        self.uniform = ops.Uniform(range=(0.0, 1.0))\n",
    "        self.flip = ops.Flip(device=\"gpu\", horizontal=1, vertical=0)\n",
    "        # self.normalize = ops.NormalizePermute(\n",
    "        #     device=\"gpu\",\n",
    "        #     mean=[0.485, 0.456, 0.406],\n",
    "        #     std=[0.229, 0.224, 0.225],\n",
    "        #     width=224,\n",
    "        #     height=224\n",
    "        # )\n",
    "        self.cmn = ops.CropMirrorNormalize(\n",
    "             device=\"gpu\",\n",
    "             output_dtype=types.FLOAT,\n",
    "        #     # output_layout=types.NCHW,\n",
    "             crop=(224, 224),\n",
    "             image_type=types.RGB,\n",
    "             mean=[0.485, 0.456, 0.406],\n",
    "             std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "\n",
    "    def define_graph(self):\n",
    "        inputs, labels = self.reader(name='Reader')\n",
    "        # output = self.flip(inputs)\n",
    "        output = self.crop(inputs, crop_pos_x=self.uniform(), crop_pos_y=self.uniform())\n",
    "#         output = self.transpose(output)\n",
    "        # flipped = self.flip(inputs)\n",
    "        # output = self.cmn(inputs)\n",
    "        return output, labels\n",
    "\n",
    "\n",
    "class DaliLoader():\n",
    "    def __init__(self, batch_size, file_root, sequence_length, crop_size, transforms=None, filenames=[]):\n",
    "        # container_files = [file_root + '/' + f for f in os.listdir(file_root)]\n",
    "\n",
    "        self.pipeline = VideoReaderPipeline(\n",
    "            batch_size=batch_size,\n",
    "            sequence_length=sequence_length,\n",
    "            num_threads=2,\n",
    "            device_id=0,\n",
    "            file_root=file_root,\n",
    "            filenames=filenames,\n",
    "            crop_size=crop_size,\n",
    "            transforms=transforms\n",
    "        )\n",
    "        self.pipeline.build()\n",
    "        self.epoch_size = self.pipeline.epoch_size('Reader')\n",
    "        self.dali_iterator = pytorch.DALIGenericIterator(\n",
    "            self.pipeline,\n",
    "            [\"data\", \"label\"],\n",
    "            self.epoch_size,\n",
    "            auto_reset=True\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.epoch_size)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.dali_iterator.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/two-stream-action-recognition/datasets/UCF101-MP4-Sample/Archery/v_Archery_g01_c06.mp4', '/two-stream-action-recognition/datasets/UCF101-MP4-Sample/Archery/v_Archery_g01_c07.mp4', '/two-stream-action-recognition/datasets/UCF101-MP4-Sample/Archery/v_Archery_g01_c02.mp4', '/two-stream-action-recognition/datasets/UCF101-MP4-Sample/Archery/v_Archery_g01_c05.mp4', '/two-stream-action-recognition/datasets/UCF101-MP4-Sample/Archery/v_Archery_g01_c01.mp4', '/two-stream-action-recognition/datasets/UCF101-MP4-Sample/Archery/v_Archery_g01_c03.mp4', '/two-stream-action-recognition/datasets/UCF101-MP4-Sample/Archery/v_Archery_g01_c04.mp4']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "file_root = Path(parent_dir) / 'datasets'/ 'UCF101-MP4-Sample'\n",
    "sequence_length = 1\n",
    "crop_size = 224\n",
    "print([str(file_root / 'Archery'/ f ) for f in os.listdir(str(file_root / 'Archery'))])\n",
    "loader = DaliLoader(\n",
    "    batch_size,\n",
    "    str(file_root),\n",
    "    sequence_length,\n",
    "    crop_size,\n",
    "    filenames=[str(file_root / 'Archery'/ f ) for f in os.listdir(str(file_root / 'Archery'))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, inputs in enumerate(loader):\n",
    "    data = inputs[0][\"data\"]\n",
    "    label = inputs[0][\"label\"]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 224, 224])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.squeeze(data).permute(0, 3, 1, 2)\n",
    "# tr = transforms.Compose([\n",
    "#     transforms.Normalize(\n",
    "#         mean=[0.485, 0.456, 0.406],\n",
    "#         std=[0.229, 0.224, 0.225]\n",
    "#     )\n",
    "# ])\n",
    "# x = tr(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# App\n",
    "from network import resnet101\n",
    "\n",
    "# Pytorch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 5e-4\n",
    "RESUME_PATH = PARENT_DIR / 'models' / 'spatial_resnet101.tar'\n",
    "NB_EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = resnet101(pretrained=True, channel=3).cuda()\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        LR,\n",
    "        momentum=0.9\n",
    "    )\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        'min',\n",
    "        patience=1,\n",
    "        verbose=True\n",
    "    )\n",
    "    return model, criterion, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, criterion, optimizer, scheduler = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resume():\n",
    "    if not RESUME_PATH.exists():\n",
    "        print(\"==> no checkpoint found at '{}'\".format(RESUME_PATH))\n",
    "        return\n",
    "    \n",
    "    checkpoint = torch.load(RESUME_PATH)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_prec1 = checkpoint['best_prec1']\n",
    "    \n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return checkpoint, start_epoch, best_prec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint, start_epoch, best_prec1 = get_resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['optimizer', 'epoch', 'best_prec1', 'state_dict'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(start_epoch, best_prec1, checkpoint['optimizer'].keys(), checkpoint['optimizer']['param_groups'])\n",
    "# print(model.parameters())\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "    start = time.time()\n",
    "    current_time = start\n",
    "    model.train()\n",
    "    \n",
    "    progress = tqdm_notebook(loader)\n",
    "    \n",
    "    for i, inputs in enumerate(progress):\n",
    "        data = inputs[0][\"data\"]\n",
    "        label = inputs[0][\"label\"].flatten()\n",
    "        \n",
    "        data_tr = torch.squeeze(data).permute(0, 3, 1, 2)\n",
    "#         data_tr = tr(data_tr)\n",
    "        \n",
    "        label = label.cuda(async=True)\n",
    "        \n",
    "        label = Variable(label).cuda().type(torch.long)\n",
    "        data_var = Variable(data_tr).cuda()\n",
    "        \n",
    "        output = model(data_var)\n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         print(input_var)\n",
    "        \n",
    "        current_time = time.time()\n",
    "#         if i > 500:\n",
    "#             break\n",
    "        \n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1051bbc2a5ac455aae8e571a07722ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3367), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data, label = train_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 2, 2, 1, 0, 0, 1, 0, 2], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> (Training video, Validation video):( 287 117 )\n"
     ]
    }
   ],
   "source": [
    "data_loader = dataloader.spatial_dataloader(\n",
    "    BATCH_SIZE=20,\n",
    "    num_workers=1,\n",
    "    path='/UCF101/jpegs/jpegs_256/',\n",
    "    ucf_list='/two-stream-action-recognition/UCF_list/',\n",
    "    ucf_split='01'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> sampling testing frames\n",
      "==> Training data : 287 frames\n",
      "torch.Size([3, 224, 224])\n",
      "==> Validation data : 2223 frames\n",
      "torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torchvision/transforms/transforms.py:209: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "train, val, test = data_loader.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in train:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 0, 0, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
